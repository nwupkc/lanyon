{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Linear Regression\n",
    "\n",
    "In this blog post, we will look at the Linear Regression model, one of the fundamental models. We will discuss two different ways to train it:\n",
    "\n",
    "- Using a Normal equation that directly computes the model parameters that best fit the model to the training set by finding the model parameters that minimize the cost function.\n",
    "- Using an iterative optimization approach called Gradient Descent that gradually tweaks the model parameters to minimize the cost function, converging to the same set of parameters as the Normal equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model makes a prediction by simply computing a weighted sum of the input variables, plus a constant called the bias term or the intercept term, as shown below.\n",
    "$$\\hat y = \\beta_0 +\\beta_1 x_1 + \\beta_2 x_2 + ⋯ + \\beta_p x_p$$\n",
    "- ŷ is the predicted value\n",
    "- p is the number of variables\n",
    "- $x_i$ is the $i^{th}$ feature value\n",
    "- $\\beta_i$ is the $i^{th}$ model parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be written much more concisely using a vectorized form, as shown below:\n",
    "$$\\hat y = \\pmb\\beta^T \\cdot \\pmb x = \\\n",
    "\\begin{bmatrix} \\beta_0 & \\beta_1 & \\cdots & \\beta_p \\end{bmatrix} \\cdot \\begin{bmatrix} 1 \\\\ x_1 \\\\ \\vdots \\\\ x_p \\end{bmatrix}$$\n",
    "\n",
    "- $\\pmb\\beta^T$ is the transpose of $\\pmb\\beta$ which is a column vector of model parameters.\n",
    "- $\\pmb x$ is a column vector, where $x_0$ equals to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a Linear Regression model, we need to find the value of $\\beta s$ that minimizes the cost function, Mean-Squared-Error (MSE). MSE cost function for a Linear Regression model looks like this:\n",
    "$$MSE = \\frac{1}{n} \\sum\\limits_{i=1}^n (\\hat y_i - y_i)^2 = \\frac{1}{n} \\sum\\limits_{i=1}^n (\\pmb\\beta^T \\cdot \\pmb x - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the \"design matrix\" $\\pmb X$ as a matrix of n rows, in which each row is the $i^{th}$ sample (the vector $\\pmb{x_i}$).\n",
    "$$\\pmb X = \\begin{bmatrix} \\pmb{x_{1}} \\\\ \\pmb{x_{2}} \\\\ \n",
    "\\vdots \\\\ \\pmb{x_{n}} \\end{bmatrix} \\\n",
    "= \\begin{bmatrix} x_{10} & x_{11} & \\cdots & x_{1p} \\\\\n",
    "x_{20} & x_{21} & \\cdots & x_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_{n0} & x_{n1} & \\cdots & x_{np} \\\\\n",
    "\\end{bmatrix}$$\n",
    "With this, we can rewrite the cost function as following, replacing the explicit sum by matrix multiplication:\n",
    "$$MSE = \\frac{1}{n} (\\pmb X \\pmb\\beta - \\pmb y)^T (\\pmb X \\pmb\\beta - \\pmb y)$$\n",
    "where $$\\pmb\\beta = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \n",
    "\\vdots \\\\ \\beta_p \\end{bmatrix},\n",
    "\\pmb y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify:\n",
    "\n",
    "$$MSE = \\frac{1}{n} ((\\pmb X \\pmb\\beta)^T - \\pmb y^T) (\\pmb X \\pmb\\beta - \\pmb y)$$\n",
    "\n",
    "$$= \\frac{1}{n} (\\pmb X \\pmb\\beta)^T \\pmb X \\pmb\\beta \\\n",
    "- (\\pmb X \\pmb\\beta)^T \\pmb y \\\n",
    "- \\pmb y^T (\\pmb X \\pmb\\beta) \\\n",
    "+ \\pmb y^T \\pmb y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\pmb{X\\beta}$ is a n x 1 vector since $\\pmb{X}$ is n x p+1 matrix and $\\pmb\\beta$ is p+1 x 1 vector and y is also a n x 1 vector. Therefore, the order does not matter, and thus we can further simplify:\n",
    "\n",
    "$$= \\frac{1}{n} \\pmb\\beta^T \\pmb X^T  \\pmb X \\pmb\\beta \\\n",
    "- 2(\\pmb X \\pmb\\beta)^T \\pmb y \\\n",
    "+ \\pmb y^T \\pmb y)$$\n",
    "\n",
    "Take first order condition w.r.t. $\\pmb\\beta$ and set it equal to zero:\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial \\pmb \\beta} = 2\\pmb X^T \\pmb X \\pmb\\beta - 2 \\pmb X^T \\pmb y = 0$$\n",
    "\n",
    "$$\\pmb X^T \\pmb X \\pmb\\beta = \\pmb X^T \\pmb y$$\n",
    "\n",
    "$$\\pmb\\beta = (\\pmb X^T \\cdot \\pmb X)^{-1} \\cdot \\pmb X^T \\cdot \\pmb y$$\n",
    "\n",
    "This is called a closed-form solution or the Normal Equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Complexity\n",
    "\n",
    "It takes:\n",
    "\n",
    "- $O(p^2n)$ to multiply $\\pmb{X^T}$ by $\\pmb{X}$\n",
    "- $O(pn)$ to multiply $\\pmb{X^T}$ by $\\pmb{y}$\n",
    "- $O(p^3)$ to invert $\\pmb{X^TX}$\n",
    "\n",
    "Asymptotically, $O(p^2n)$ dominates $O(pn)$ so we can forget the $O(pn)$ part. We will assume that n > p since otherwise the matrix $\\pmb{X^TX}$ would be singular or degenerate (and hence non-invertible), which means that $O(p^2n)$ asymptotically dominates $O(p^3)$.\n",
    "\n",
    "Therefore the total time complexity is $O(p^2n)$. \n",
    "\n",
    "Disregard for now that n > p. The Normal Equation computes the inverse of $\\pmb{X^TX}$ which has computational complexity is $O(p^3)$. This means that if you double the number of variables, you are multiplying the computation time by 8 times. Therefore, the Normal Equation gets very slow when the number of variables is large.\n",
    "\n",
    "On the other hand, this equation is linear with regards to the number of observations in the training set (i.e., $O(n)$), so it handles large training sets efficiently, provided they can fit in memory.\n",
    "\n",
    "Also, once you have trained your Linear Regression model, predictions are very fast: the computational complexity is linear with regards to both the number of observations you want to make predictions on and the number of variables since it takes $O(pn)$ to multiply $\\pmb{\\beta}$ by $\\pmb{X}$. Making predictions on twice as many observations or twice as many variables will take twice as much time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Now we will consider a different way to train a Linear Regression model which is better suited for cases where there are a large number of variables, or too many training observations to fit in memory.\n",
    "\n",
    "_Gradient Descent_ is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is Gradient__ the gradient is a multi-variable generalization of the derivative. While a derivative can be defined on functions of a single variable, for functions of several variables, the gradient takes its place. The gradient is a vector-valued function, as opposed to a derivative, which is scalar-valued.\n",
    "\n",
    "Like the derivative, the gradient represents the slope of the tangent of the graph of the function. More precisely, the gradient points in the direction of the greatest rate of increase of the function, and its magnitude is the slope of the graph in that direction. The components of the gradient in coordinates are the coefficients of the variables in the equation of the tangent space to the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent measures the local gradient of the error function with regards to the parameter vector $\\beta s$, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concretely, you start by filling $\\beta$ with random values (this is called random initialization), and then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important parameter in Gradient Descent is the size of the steps, determined by the learning rate hyper-parameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time.\n",
    "\n",
    "On the other hand, if the learning rate is too high, you might jump across the valley and end up on the other side, possibly even higher up than you were before. This might make the algorithm diverge, with larger and larger values, failing to find an optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convexity means that if you pick any two points on the curve, the line segment joining them never crosses the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high). Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function.\n",
    "\n",
    "But not all cost functions are nice convex functions. There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum very difficult. If the random initialization starts the algorithm on some places, then it will converge to a local minimum rather than the global minimum. If it starts on the plateau, it will take a very long time to cross and if you stop prematurely, you will never reach the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using Gradient Descent, you should ensure that all features have a similar scale or else it will take much longer to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model means searching for a combination of model parameters that minimizes a cost function. It is a search in the model’s parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is: searching for a needle in a 300-dimensional haystack is much trickier than in three dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
