---
layout: post
title: The Bias-Variance Trade-Off
---


The bias–variance trade-off is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set.

__What is bias__ The bias is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The bias refers to the error that is introduced by approximating a complicated, real-life problem by a much simpler model. For example, linear regression assumes that there is a linear relationship between $Y$ and $X_1, X_2, . . . , X_p$. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of the true relationship. If the true relationship is substantially non-linear, so no matter how many training observations we are given, it will not be possible to produce an accurate estimate using linear regression. In other words, linear regression results in high bias in this example. If the true f is very close to linear, and so given enough data, it should be possible for linear regression to produce an accurate estimate. Generally, more flexible methods result in less bias.

__What is variance__ The variance is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). The variance refers to the amount by which the predicted output would change if we estimated it using a different training data set. Ideally the estimate for output should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in predictions. In general, more flexible statistical methods have higher variance. It has high variance because changing any one of these data points may cause the estimated outputs to change considerably. In contrast, the linear regression line is relatively inflexible and has low variance, because moving any single observation will likely cause only a small shift in the position of the line.

The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself. As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test error increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test error declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test error increases.

The relationship between bias, variance, and test set error is referred to as the bias-variance trade-off. Good test set performance of a statistical learning method requires low variance as well as low squared bias. This is referred to as a trade-off because it is easy to obtain a method with extremely low bias but high variance (for instance, by drawing a curve that passes through every single training observation) or a method with very low variance but high bias (by fitting a horizontal line to the data). The challenge lies in finding a method for which both the variance and the squared bias are low.
